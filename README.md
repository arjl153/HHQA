## Method to Run
- Run the python file calculate_result_Hindi.py with the below commandline arguments:
-   response_file_name LLM_GPT-4_Responses.json
-   api_key
-   organization

## Introduction:
- Introduces "Hindi Hallucination QA (HHQA)" benchmark for evaluating hallucinatory tendencies in Large Language Models (LLMs) like GPT-4 and LLaMA-3 within the Hindi language context.
- Aims to challenge models beyond conventional parameters by focusing on imitative misconceptions and factual inaccuracies.
- Initial findings reveal significant variability in performance, with GPT-4 demonstrating a non-hallucination rate of 63.64%.
- Large Language Models (LLMs) have revolutionized natural language processing but frequently exhibit hallucinations, generating factually incorrect or nonsensical responses.
- HHQA dataset aims to evaluate and mitigate such errors in less represented languages like Hindi.

## The Hindi HalluQA Benchmark:
- Criteria tailored to address hallucination tendencies in Hindi-speaking regions.
- Dataset construction guided by adversarial prompts challenging models with cultural nuances.

## Methodology:
- Response generation, input preparation, model interaction, data collection, quality control, and validation outlined.
- Evaluation involves comparing responses against predefined truthfulness criteria.

## Experiments:
- Focus on evaluating non-hallucination rates of GPT-4 and LLaMA-3 in detecting hallucinations within Hindi responses.

## Result:
- Preliminary results show significant non-hallucination rate for GPT-4, variable performance for LLaMA-3.

## Observations and Insights:
- GPT-4 outperforms LLaMA-3 in certain categories, highlighting the need for robust benchmarks in underrepresented languages.

## Conclusion:
- Research contributes to enhancing the trustworthiness of language models in Hindi and paves the way for further linguistic-inclusive AI research.
